# ABSorBEnT

The entire data set utilized in ABSorBENT including all evaluation tables, graphs, etc.

Abstract: 
Large Language Models (LLMs) have evolved from simple “chatbots”, to AI assistants that are capable of understanding and communicating a variety of technical subjects. In particular, LLMs have the potential to aid researchers studying synthetic biology, molecular biology, organic chemistry, or biochemistry. However, it is still unclear what types of biochemistry tasks are best delegated to LLMs. I present ABSorBEnT: Ai Benchmarking Suite fOR Biochemical ENgineering Tasks. ABSorBEnT is a simple framework that aims to understand how LLMs reason accurately with synthetic biology concepts by testing prominent models ChatGPT-3.5 and Bard with questions from 3 different bioengineering domains and question difficulties. Amongst the many conclusions I’ve gathered, overall, the results show that Bard is 5% more accurate than ChatGPT. My findings also reveal that LLMs are best at performing “textbook look-up” questions, rather than generative application based questions. While these insights offer strong initial conclusions, a larger and more complete evaluation set would offer a compelling supplemental discussion. All of the evaluation data used in ABSorBEnT is publicly available at: github.com/TaraSPande/ABSorBEnT.
